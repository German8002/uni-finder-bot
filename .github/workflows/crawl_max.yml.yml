name: crawl-max

on:
  workflow_dispatch:
    inputs:
      mode:
        description: "Run mode"
        required: true
        type: choice
        options:
          - quick   # только преобразование CSV -> JSON и коммит
          - full    # полный сбор Wikipedia + конвертация и коммит
        default: quick
  schedule:
    - cron: '0 3 * * 6'  # каждую субботу 03:00 UTC — всегда full

permissions:
  contents: write

concurrency:
  group: crawl-max
  cancel-in-progress: false

jobs:
  build_universities:
    name: Build universities (Wikipedia) · ${{ inputs.mode || 'scheduled' }}
    runs-on: ubuntu-latest
    timeout-minutes: 420
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - name: Install deps
        run: pip install -r requirements.txt

      # Полный сбор (только если schedule ИЛИ выбран режим full)
      - name: Full crawl (Wikipedia) with heartbeat
        if: ${{ github.event_name == 'schedule' || inputs.mode == 'full' }}
        env:
          PYTHONUNBUFFERED: "1"
        run: |
          set -e
          python -u scraper_max/scripts/build_universities.py &
          PID=$!
          while kill -0 "$PID" 2>/dev/null; do
            echo "[heartbeat] build_universities.py still running $(date -u +'%Y-%m-%dT%H:%M:%SZ')"
            sleep 60
          done
          wait "$PID"

      - name: Upload universities.csv artifact (backup)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: universities_csv
          path: scraper_max/out/universities.csv
          if-no-files-found: warn

      - name: Inspect CSV head (debug)
        run: |
          echo "--- CSV head ---"
          head -n 12 scraper_max/out/universities.csv || true
          echo "----------------"

      # Конвертация CSV -> JSON выполняется и в quick, и в full
      - name: Convert CSV -> data/universities.json
        run: |
          python - <<'PY'
          import csv, json, os, sys, codecs
          p = "scraper_max/out/universities.csv"
          if not os.path.exists(p):
            print("No CSV found at", p, "- nothing to convert")
            sys.exit(0)
          with open(p, "rb") as fh:
            raw = fh.read()
          try:
            text = raw.decode("utf-8-sig")
          except:
            text = raw.decode("utf-8", "ignore")
          rows = list(csv.DictReader(text.splitlines()))
          os.makedirs("data", exist_ok=True)
          with open("data/universities.json", "w", encoding="utf-8") as out:
            json.dump(rows, out, ensure_ascii=False, indent=2)
          print("JSON items:", len(rows))
          PY

      - name: Commit updated CSV+JSON
        run: |
          set -e
          git config --global user.name "github-actions"
          git config --global user.email "github-actions@github.com"
          git add scraper_max/out/universities.csv data/universities.json || true
          git diff --staged --quiet && echo "No changes to commit" || git commit -m "data: update universities.csv + data/universities.json [skip ci]"
          git pull --rebase --autostash
          git push
