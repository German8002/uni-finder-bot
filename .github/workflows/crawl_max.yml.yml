name: crawl-max

on:
  workflow_dispatch:
    inputs:
      mode:
        description: "Run mode: quick (convert only) or full (crawl + convert)"
        required: false
        default: "quick"
  schedule:
    - cron: '0 3 * * 6'  # weekly, Saturday 03:00 UTC

permissions:
  contents: write

concurrency:
  group: crawl-max
  cancel-in-progress: false

jobs:
  build_universities:
    name: Build universities (Wikipedia) with heartbeat
    runs-on: ubuntu-latest
    timeout-minutes: 420  # 7h hard limit to avoid auto-cancel
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install deps
        if: ${{ github.event.inputs.mode != 'quick' }}
        run: pip install -r requirements.txt

      - name: Build universities (Wikipedia) with heartbeat
        if: ${{ github.event.inputs.mode != 'quick' }}
        env:
          PYTHONUNBUFFERED: "1"
        run: |
          set -e
          python -u scraper_max/scripts/build_universities.py &
          PID=$!
          while kill -0 "$PID" 2>/dev/null; do
            echo "[heartbeat] build_universities.py still running $(date -u +'%Y-%m-%dT%H:%M:%SZ')"
            sleep 60
          done
          wait "$PID"

      - name: Upload universities.csv artifact (backup)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: universities_csv
          path: scraper_max/out/universities.csv
          if-no-files-found: warn

      - name: Inspect CSV head (debug)
        run: |
          echo "--- CSV head ---"
          head -n 12 scraper_max/out/universities.csv || true

      - name: Convert CSV -> data/universities.json
        run: |
          set -e
          python - <<'PY'
          import csv, json, os, re, sys, io

          csv_path = "scraper_max/out/universities.csv"
          out_dir = "data"
          out_path = os.path.join(out_dir, "universities.json")
          os.makedirs(out_dir, exist_ok=True)

          if not os.path.exists(csv_path):
              print(f"NO_CSV: {csv_path} not found")
              sys.exit(0)

          raw = open(csv_path, "rb").read()

          # Try a few common encodings (CSV from wiki can be tricky)
          text = None
          for enc in ("utf-8-sig", "utf-8", "cp1251"):
              try:
                  text = raw.decode(enc)
                  print(f"Decoded CSV with encoding: {enc}")
                  break
              except Exception:
                  pass
          if text is None:
              text = raw.decode("utf-8", "ignore")
              print("Decoded CSV with encoding: utf-8 (errors ignored)")

          # Detect delimiter safely
          sample = "\n".join(text.splitlines()[:50])
          try:
              dialect = csv.Sniffer().sniff(sample)
              delim = getattr(dialect, "delimiter", ",")
          except Exception:
              dialect = csv.excel
              delim = ","
          print(f"Detected delimiter: {delim!r}")

          reader = csv.DictReader(io.StringIO(text), dialect=dialect)

          def norm_homepage(v: str|None) -> str|None:
              if not v:
                  return None
              v = v.strip()
              if not v:
                  return None
              if v.startswith("//"):
                  v = "https:" + v
              return v

          def norm_pageid(v) -> int|None:
              if v is None or v == "":
                  return None
              s = re.sub(r"\\D+", "", str(v))
              if not s:
                  return None
              try:
                  return int(s)
              except Exception:
                  return None

          items = []
          for i, row in enumerate(reader, 1):
              # allow flexible headers
              title = row.get("title") or row.get("Title") or row.get("name") or row.get("Name")
              homepage = row.get("homepage") or row.get("site") or row.get("url")
              pageid = row.get("pageid") or row.get("page_id") or row.get("PageID")

              title = (title or "").strip()
              homepage = norm_homepage(homepage)
              pageid = norm_pageid(pageid)

              if not title and pageid is None and homepage is None:
                  # completely empty logical row -> skip
                  continue

              items.append({
                  "title": title or None,
                  "homepage": homepage,
                  "pageid": pageid,
              })

          with open(out_path, "w", encoding="utf-8") as f:
              json.dump(items, f, ensure_ascii=False, indent=2)

          print(f"Wrote {len(items)} items -> {out_path}")
          PY

      - name: Show JSON stats
        run: |
          python - <<'PY'
          import json, os
          p = "data/universities.json"
          if os.path.exists(p):
              d = json.load(open(p, encoding="utf-8"))
              print("JSON items:", len(d))
          else:
              print("JSON not found")
          PY

      - name: Commit updated CSV+JSON
        run: |
          set -e
          git config --global user.name "github-actions"
          git config --global user.email "github-actions@github.com"
          git add -A
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "data: update universities.csv + data/universities.json [skip ci]"
            git pull --rebase --autostash
            git push
          fi
