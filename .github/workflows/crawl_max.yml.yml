
name: crawl-max

on:
  workflow_dispatch:
    inputs:
      mode:
        description: "Run mode: full (crawl) or quick (convert only)"
        required: false
        default: "quick"
  schedule:
    - cron: '0 3 * * 6'  # weekly, Saturday 03:00 UTC

permissions:
  contents: write

concurrency:
  group: crawl-max
  cancel-in-progress: false

jobs:
  build_universities:
    name: Build universities (Wikipedia) with heartbeat
    runs-on: ubuntu-latest
    timeout-minutes: 420  # 7h hard limit to avoid auto-cancel
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install deps
        run: |
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          fi
          # pandas only if available elsewhere; we don't require it for convert

      - name: Crawl (full) with heartbeat
        if: ${{ github.event.inputs.mode != 'quick' }}
        env:
          PYTHONUNBUFFERED: "1"
        run: |
          set -e
          python -u scraper_max/scripts/build_universities.py &
          PID=$!
          while kill -0 "$PID" 2>/dev/null; do
            echo "[heartbeat] build_universities.py still running $(date -u +'%Y-%m-%dT%H:%M:%SZ')"
            sleep 60
          done
          wait "$PID"

      - name: Upload universities.csv artifact (backup)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: universities_csv
          path: scraper_max/out/universities.csv
          if-no-files-found: warn

      - name: Inspect CSV head (debug)
        run: |
          echo "--- CSV head ---"
          head -n 10 scraper_max/out/universities.csv || true
          echo "----------------"

      - name: Convert CSV -> data/universities.json
        shell: bash
        run: |
          set -e
          PY=$(cat <<'PYCODE'
          import csv, json, os, sys, re
          from pathlib import Path

          csv_path = Path("scraper_max/out/universities.csv")
          out_path = Path("data/universities.json")
          out_path.parent.mkdir(parents=True, exist_ok=True)

          if not csv_path.exists():
            print(f"No CSV found at {csv_path}, skipping JSON write")
            sys.exit(0)

          # Robust reader (handles BOM, autodetects delimiter, tolerates missing fields)
          encodings = ["utf-8-sig", "utf-8", "cp1251"]
          for enc in encodings:
            try:
              with open(csv_path, "r", encoding=enc, newline="") as f:
                sample = f.read(4096)
                f.seek(0)
                # Try to sniff delimiter; default to comma
                try:
                  sniffer = csv.Sniffer()
                  dialect = sniffer.sniff(sample, delimiters=[",",";","\t","|"])
                except csv.Error:
                  class _D: delimiter = ","
                  dialect = _D()
                reader = csv.DictReader(f, delimiter=getattr(dialect, "delimiter", ","))
                headers = [h.strip().lower() for h in (reader.fieldnames or [])]
                print("Headers:", headers)
                # normalize header mapping
                def pick(row, keys):
                  for k in keys:
                    if k in row and row[k]:
                      return row[k]
                  return ""

                items = []
                seen = set()
                for row in reader:
                  # normalize keys to lowercase
                  row = { (k or "").strip().lower(): (v or "").strip() for k, v in row.items() }
                  title = pick(row, ["title","name","label","университет","вуз"])
                  pageid = pick(row, ["pageid","id","wiki_id"])
                  homepage = pick(row, ["homepage","url","site","website","сайт"])

                  if not title and not pageid:
                    # empty row
                    continue

                  # clean homepage (allow empty)
                  if homepage and not re.match(r"^https?://", homepage, re.I):
                    # sometimes wikipedia list contains //domain.tld
                    if homepage.startswith("//"):
                      homepage = "https:" + homepage
                    else:
                      # if it looks like domain.tld add http
                      if re.match(r"^[\w.-]+\.[a-z]{2,}$", homepage, re.I):
                        homepage = "http://" + homepage
                      else:
                        # keep as-is or blank if clearly garbage
                        pass

                  # coerce pageid to int when possible
                  try:
                    pageid_int = int(float(pageid)) if pageid else None
                  except Exception:
                    pageid_int = None

                  key = (title.lower(), pageid_int)
                  if key in seen:
                    continue
                  seen.add(key)

                  items.append({
                    "title": title,
                    "homepage": homepage or None,
                    "pageid": pageid_int,
                  })

                print(f"Parsed rows: {len(items)}")
                if not items:
                  print("No rows parsed — JSON not updated")
                  sys.exit(0)

                with open(out_path, "w", encoding="utf-8") as out:
                  json.dump(items, out, ensure_ascii=False, indent=2)
                print(f"Wrote JSON -> {out_path}")

                break
            except Exception as e:
              last_err = e
          else:
            print(f"Failed to read CSV with tried encodings; last error: {last_err!r}")
            sys.exit(0)
          PYCODE
          )
          python - <<'PY' "$PY"
          PY

      - name: Commit updated CSV+JSON
        run: |
          set -e
          git config --global user.name "github-actions"
          git config --global user.email "github-actions@github.com"
          git add scraper_max/out/universities.csv data/universities.json || true
          git diff --staged --quiet && echo "No changes to commit" || git commit -m "data: update universities.json (+csv) [skip ci]"
          git pull --rebase --autostash
          git push
