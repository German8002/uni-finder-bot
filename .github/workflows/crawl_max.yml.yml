name: crawl-max

on:
  workflow_dispatch:
    inputs:
      mode:
        description: 'Run mode'
        required: true
        default: 'quick'
        type: choice
        options: [full, quick]
  schedule:
    - cron: '0 3 * * 6'  # weekly

permissions:
  contents: write

concurrency:
  group: crawl-max
  cancel-in-progress: false

jobs:
  build_universities:
    runs-on: ubuntu-latest
    timeout-minutes: 420
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with: { python-version: '3.11' }
      - name: Install deps
        run: pip install -r requirements.txt

      # ⬇️ Полный краул запускается ТОЛЬКО когда mode == full
      - name: Build universities (Wikipedia) with heartbeat
        if: ${{ inputs.mode == 'full' || github.event_name == 'schedule' }}
        env:
          PYTHONUNBUFFERED: "1"
        run: |
          set -e
          python -u scraper_max/scripts/build_universities.py &
          PID=$!
          while kill -0 "$PID" 2>/dev/null; do
            echo "[heartbeat] build_universities.py still running $(date -u +'%Y-%m-%dT%H:%M:%SZ')"
            sleep 60
          done
          wait "$PID"

      - name: Upload universities.csv artifact (backup)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: universities_csv
          path: scraper_max/out/universities.csv
          if-no-files-found: warn

      # Конвертация CSV -> JSON работает и для full, и для quick
      - name: Convert CSV -> data/universities.json
        run: |
          python - << 'PY'
          import csv, json, pathlib
          IN, OUT = "scraper_max/out/universities.csv", "data/universities.json"
          rows=[]
          try:
            with open(IN, newline="", encoding="utf-8") as f:
              for r in csv.DictReader(f):
                name=(r.get("name") or r.get("Название") or "").strip()
                city=(r.get("city") or r.get("Город") or "").strip()
                site=(r.get("site") or r.get("Сайт") or r.get("url") or "").strip()
                if name: rows.append({"name":name,"city":city,"site":site,"source":"wikipedia"})
          except FileNotFoundError:
            print(f"WARNING: {IN} not found; leaving JSON unchanged if exists.")
            rows=[]
          # dedup
          seen=set(); uniq=[]
          for r in rows:
            k=(r["name"].lower(), r["city"].lower())
            if k in seen: continue
            seen.add(k); uniq.append(r)
          pathlib.Path("data").mkdir(parents=True, exist_ok=True)
          if rows:
            with open(OUT,"w",encoding="utf-8") as f: json.dump(uniq,f,ensure_ascii=False)
            print(f"Wrote {len(uniq)} rows to {OUT}")
          else:
            # если quick-режим и CSV нет, просто не трогаем JSON
            print("No CSV — skipping JSON write")
          PY

      - name: Commit updated CSV+JSON
        run: |
          set -e
          git config --global user.name "github-actions"
          git config --global user.email "github-actions@github.com"
          git add scraper_max/out/universities.csv data/universities.json || true
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "chore(data): update universities (csv+json) [skip ci]"
            git pull --rebase --autostash
            git push
          fi
