name: crawl-max

on:
  workflow_dispatch:
    inputs:
      mode:
        description: "Run mode: full (crawl) or quick (just convert+commit if CSV exists)"
        required: false
        default: "full"
  schedule:
    - cron: '0 3 * * 6'  # weekly, Saturday 03:00 UTC

permissions:
  contents: write

concurrency:
  group: crawl-max
  cancel-in-progress: false

jobs:
  build_universities:
    name: Build universities (Wikipedia) with heartbeat
    runs-on: ubuntu-latest
    timeout-minutes: 420  # 7h hard limit
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install deps
        run: pip install -r requirements.txt

      - name: Build universities (Wikipedia) with heartbeat
        if: ${{ github.event.inputs.mode != 'quick' }}
        env:
          PYTHONUNBUFFERED: "1"
        run: |
          set -e
          python -u scraper_max/scripts/build_universities.py &
          PID=$!
          while kill -0 "$PID" 2>/dev/null; do
            echo "[heartbeat] build_universities.py still running $(date -u +'%Y-%m-%dT%H:%M:%SZ')"
            sleep 60
          done
          wait "$PID"

      - name: Ensure CSV exists (quick mode ok)
        run: |
          if [ ! -f scraper_max/out/universities.csv ]; then
            echo "ERROR: scraper_max/out/universities.csv not found."
            echo "If you ran in 'quick' mode, make sure the CSV already exists in the repo (or rerun in 'full')."
            exit 1
          fi
          wc -l scraper_max/out/universities.csv || true

      - name: Upload universities.csv artifact (backup)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: universities_csv
          path: scraper_max/out/universities.csv
          if-no-files-found: warn

      - name: Inspect CSV head (debug)
        run: |
          echo "--- CSV head ---"
          head -n 5 scraper_max/out/universities.csv || true
          echo "----------------"

      - name: Convert CSV -> data/universities.json
        run: |
          python - << 'PY'
          import csv, json, pathlib

          IN  = "scraper_max/out/universities.csv"
          OUT = "data/universities.json"

          rows = []
          headers = []

          def pick(d, *keys):
              for k in keys:
                  v = d.get(k)
                  if v:
                      return v
              return ""

          try:
              with open(IN, encoding="utf-8", newline="") as f:
                  sample = f.read(8192)
                  f.seek(0)
                  try:
                      dialect = csv.Sniffer().sniff(sample, delimiters=",;\\t|")
                  except csv.Error:
                      dialect = csv.get_dialect("excel")
                  reader = csv.DictReader(f, dialect=dialect)
                  headers = [h.strip().lower() for h in (reader.fieldnames or [])]

                  for r in reader:
                      d = { (k or "").strip().lower(): (v or "").strip() for k,v in r.items() }

                      name = pick(d, "name","название","университет","вуз")
                      city = pick(d, "city","город","населенный пункт","населённый пункт","место")
                      site = pick(d, "site","сайт","url","веб-сайт","вебсайт")

                      if name:
                          rows.append({
                              "name": name,
                              "city": city,
                              "site": site,
                              "source": "wikipedia"
                          })
          except FileNotFoundError:
              print(f"WARNING: {IN} not found")

          print(f"Headers: {headers}")
          print(f"Parsed rows: {len(rows)}")

          seen, uniq = set(), []
          for r in rows:
              k = (r["name"].lower(), (r["city"] or "").lower())
              if k in seen:
                  continue
              seen.add(k)
              uniq.append(r)

          pathlib.Path("data").mkdir(parents=True, exist_ok=True)
          if uniq:
              with open(OUT, "w", encoding="utf-8") as f:
                  json.dump(uniq, f, ensure_ascii=False, indent=2)
              print(f"Wrote {len(uniq)} rows to {OUT}")
          else:
              print("No rows parsed — JSON not updated")
          PY

      - name: Commit updated CSV+JSON
        run: |
          set -e
          git config --global user.name "github-actions"
          git config --global user.email "github-actions@github.com"
          git add scraper_max/out/universities.csv data/universities.json || true
          git diff --staged --quiet && echo "No changes to commit" || git commit -m "data: update universities [skip ci]"
          git pull --rebase --autostash
          git push
