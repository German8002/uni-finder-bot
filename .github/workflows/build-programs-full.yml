name: crawl-programs-full

on:
  workflow_dispatch:
  schedule:
    - cron: '0 4 * * 6'  # weekly, Saturday 04:00 UTC

permissions:
  contents: write

concurrency:
  group: crawl-programs
  cancel-in-progress: false

jobs:
  build_programs:
    name: Build programs (full) with heartbeat
    runs-on: ubuntu-latest
    # NOTE: GitHub-hosted runners have a hard cap (~6h). Setting a bigger
    # timeout here only protects from shorter auto-cancels.
    timeout-minutes: 600  # 10h
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install deps
        run: pip install -r requirements.txt

      - name: Build programs (Wikipedia) with heartbeat (full)
        env:
          PYTHONUNBUFFERED: "1"
        run: |
          set -e
          # Your scraper must write CSV to scraper_max/out/programs.csv
          python -u scraper_max/scripts/build_programs.py --mode full --out scraper_max/out/programs.csv &
          PID=$!
          while kill -0 "$PID" 2>/dev/null; do
            echo "[heartbeat] build_programs.py still running $(date -u +'%Y-%m-%dT%H:%M:%SZ')"
            sleep 60
          done
          wait "$PID"

      - name: Upload programs.csv artifact (backup)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: programs_csv
          path: scraper_max/out/programs.csv
          if-no-files-found: warn

      - name: Convert CSV -> data/programs_latest.json
        run: |
          python - << 'PY'
          import csv, json, sys, pathlib
          p = pathlib.Path("scraper_max/out/programs.csv")
          if not p.exists():
              print("No CSV — skipping JSON write")
              sys.exit(0)
          # Read text and sniff delimiter
          text = p.read_text(encoding="utf-8-sig", errors="ignore")
          lines = [ln for ln in text.splitlines() if ln.strip()]
          # Fallback delimiter if sniffer fails
          delim = ','
          try:
              dialect = csv.Sniffer().sniff('\n'.join(lines[:10]))
              delim = dialect.delimiter
          except Exception:
              pass
          reader = csv.DictReader(lines, delimiter=delim)
          rows = list(reader)
          print(f"Parsed rows: {len(rows)}")
          out = []
          for r in rows:
              out.append({
                  "title": r.get("title") or r.get("program") or r.get("name"),
                  "university": r.get("university") or r.get("org") or r.get("institute"),
                  "city": r.get("city") or r.get("город"),
                  "level": r.get("level") or r.get("degree") or r.get("уровень"),
                  "form": r.get("form") or r.get("education_form") or r.get("форма"),
                  "duration": r.get("duration") or r.get("срок"),
                  "url": r.get("url") or r.get("homepage") or r.get("link"),
                  "pageid": r.get("pageid"),
              })
          dest = pathlib.Path("data/programs_latest.json")
          dest.parent.mkdir(parents=True, exist_ok=True)
          dest.write_text(json.dumps(out, ensure_ascii=False, indent=2), encoding="utf-8")
          print(f"Wrote {len(out)} items -> {dest}")
          PY

      - name: Commit updated programs JSON
        run: |
          set -e
          git config --global user.name "github-actions"
          git config --global user.email "github-actions@github.com"
          git add data/programs_latest.json || true
          git diff --staged --quiet && echo "No changes to commit" || git commit -m "data: update programs_latest.json [skip ci]"
          git pull --rebase --autostash
          git push
